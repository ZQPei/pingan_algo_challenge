# 平安算法数据挖掘大赛代码分享

### 使用说明

- 把train.csv,test.csv放入data目录下。
- 运行src目录中的main.py，以及main_easy_ensemble.py（这是对main做的改进，用到easy_ensemble方法）。
- 所有参数的设置都放在main.py的22~100行。

### 算法说明

- 首先对原始数据进行预处理，相关函数在pre_process.py中。

  这一步至关重要！提醒那些f2值低的同学以及自己，千万不能随便删特征！！！

  宁愿丢弃部分样本，也不能丢特征！！！

  原始数据中缺失值我是用填充-1或者0来处理的，发现填充-1效果好一些。

- 接着，将train.csv读取的data_frame中的数据集划分成train set和test set，用到sklearn中的train_test_split函数，比例可以自己调节，建议0.33或者0.2。

  这里要强调一点，不少同学在划分数据集之前就对数据进行采样处理了，千万不能这么做！不然就很容易发现训练后的f2值在自己的test set上很高，但是线上的f2分数就很低。

- 下一步，对train set的数据进行降采样或者上采样处理，因为是不平衡数据，200:1，所以各种上采样和下采样方法都试了。但是发现对于这个数据集SMOTE上采样算法不是特别好使，所以最后只用了下采样。

- 对采样后的train set进行分类，用到的分类算法是XGBOOST，应该大部分人都是用这个。其中调参的滋味就自己体会，主要还是防止过拟合和欠拟合吧。

  然后，我对于调参的看法是，调参可以提高模型的性能，但是真的不大，可以从30%上升到35%，但是一定不会从30%突然上升到50%+，**有一个好的数据集比调参更重要，所以处理数据集要耐心**。

- 最后，将训练的模型在test set上进行测试，我们的模型最好只能达到35%。



### 总结

第一次参加比赛，感觉还是付出了比较多努力的，第一次用机器学习的库sklearn，学到了很多数据处理的技巧，而且每一次有了新的想法都想马上用代码验证，希望f2值突然涨到50%+，但是每次都在原地徘徊。我不知道其他人是怎么样的，反正这个比赛我尽力了，希望观摩一下大神的代码和算法~~~




# pingan_data_mining
